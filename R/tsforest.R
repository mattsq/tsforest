#' Time Series Forest
#'
#' Train a time series forest model.
#'
#' This function trains a time series classification forest model using the methods
#' described by Deng et al. (2013). Specifically, the code here is based on
#' the description at:
#' http://www.timeseriesclassification.com/algorithmdescription.php?algorithm_id=5
#' In essence, the model splits each time series into `sqrt(ncol(df))` intervals,
#' then calcuates the mean, standard deviation and slope of those intervals, and
#' then uses those values for every interval as the inputs to a random forest model.
#' Here, we use `ranger::`.
#'
#' @param df A data.frame object where each row is a time series.
#' @param target The name of the predicted class column, as a string
#' @param min_length The minimum length of the sample intervals created. Must be >= 2
#' @param verbose Whether or not to print the state of training as it happens.
#' @param ... arguments to be passed to the `ranger::ranger()`model, like mtry
#' @return Returns a time series forest model object (just a list at the moment)
#' @examples
#' \dontrun{
#' data("LargeKitchenAppliances_TRAIN")
#' model <- tsforest(LargeKitchenAppliances_TRAIN, target = "target")
#' }
#' @importFrom purrr map_dbl
#' @import ranger
#' @importFrom stats as.formula
#' @export
tsforest <- function(df,
                     target = "target",
                     min_length = 2,
                     verbose = TRUE,
                     ...) {
  X_df <- df[,!colnames(df) == target]

  n_intervals <- floor(sqrt(ncol(X_df)))
  if (verbose) {
    cat(glue::glue("Training model with {n_intervals} intervals..."))
    cat("\n")
  }
  returned_object <- structure(list(
    training_df = NA,
    featurized_df = NA,
    ranger_model = NA,
    target = target,
    intervals = list(
      start = numeric(n_intervals),
      end = numeric(n_intervals)
    )
  ), class = "tsforest")
  returned_object$training_df <- df

  returned_object$intervals$start <- sample(1:((ncol(X_df)-min_length)), n_intervals)
  returned_object$intervals$end <- purrr::map_dbl(returned_object$intervals$start, ~ sample((.x+min_length):ncol(X_df), 1))

  featurized_df <- featurize_df(X_df = X_df,
                                returned_object = returned_object,
                                verbose = verbose)
  featurized_df$target <- df[,colnames(df) == target]
  returned_object$featurized_df <- featurized_df

  form_for_pred <- stats::as.formula(paste0(target, " ~ ."))

  returned_object$ranger_model <- ranger::ranger(form_for_pred,
                                                 data = returned_object$featurized_df,
                                                 ...)
  return(returned_object)
}

#' Predict Time Series Forest
#'
#' Predict classes using a time series forest model
#'
#' This function takes a time series forest model and returns predictions, and
#' featurizes new data using the same intervals as needed.
#'
#' @param model A time series forest model object
#' @param newdata optional new data frame - if not supplied, will use training data
#' @param verbose Whether to print state of featurizing new data
#' @param ... arguments to be passed to `ranger::predict.ranger`, like `type = "response"`
#' @return Returns predictions in the form of a `ranger::predict.ranger` response.
#' @examples
#' \dontrun{
#' data("LargeKitchenAppliances_TRAIN")
#' data("LargeKitchenAppliances_TEST")
#' model <- tsforest(LargeKitchenAppliances_TRAIN, target = "target")
#' train_preds <- predict_tsforest(model)
#' test_preds <- predict_tsforest(model, newdata = LargeKitchenAppliances_TEST)
#' }
#'
#' @importFrom stats predict
#' @export
predict.tsforest <- function(model,
                             newdata = NULL,
                             verbose = TRUE,
                             ...) {
  if(is.null(newdata)) {
    preds <- stats::predict(model$ranger_model, data = model$featurized_df, type = type)
  } else {
    if (verbose) cat("Fitting new data to trained intervals:\n")
    X_newdata <- newdata[,!colnames(newdata) == model$target]
    featurized_newdata <- featurize_df(X_newdata, model, verbose = verbose)
    preds <- stats::predict(model$ranger_model, data = featurized_newdata, ...)
  }
  return(preds)
}

#' Plot Interval-Wise Variable Importance
#'
#' Create a ggplot summarising the variable importance over intervals
#'
#' This function creates a ggplot that summarizes the importance of various
#' interval features generated by the model over the interval. It can only
#' be used on models where `ranger::ranger(..., importance = '')` has been set.
#' A summary function must also be passed - sum and mean seem to have the best
#' outcomes. Optionally, you may pass a row index to visualise an individual time
#' series over the top of the variable importance.
#' @param model a model object generated by `tsforest::tsforest()`
#' @param summary_function any summary function that returns a single value
#' @param optional_example_rownumber an optional rownumber of a time series from the training set
#' @param ... any arguments to be passed to the summary function
#' @examples
#' data("LargeKitchenAppliances_TRAIN")
#' model <- tsforest::tsforest(LargeKitchenAppliances_TRAIN, verbose = FALSE, importance = 'impurity')
#' intervalwise_variable_importance(model, summary_function = sum)
#' @importFrom dplyr bind_rows group_by summarise
#' @importFrom ggplot2 ggplot aes geom_line
#' @importFrom tidyr unnest
#' @importFrom purrr map2
#' @importFrom tibble tibble
#' @importFrom scales rescale
#' @export
intervalwise_variable_importance <- function(model,
                                             summary_function = NULL,
                                             optional_example_rownumber = NULL,
                                             ...) {
  if (model$ranger_model$importance.mode == "none") {
    stop("Can only be passed if trained with variable importance.")
  }
  if (is.null(summary_function)) {
    stop("Please provide a summary function to be passed to summarise, without brackets.")
  }
  if (!is.null(optional_example_rownumber)) {
    if (optional_example_rownumber > nrow(model$training_df)) {
    stop("Example rownumber isn't in the training dataframe.")
    }
  }

  scores <- model$ranger_model$variable.importance
  mean_scores <- scores[grep("mean", names(scores))]
  sd_scores <- scores[grep("sd", names(scores))]
  slope_scores <- scores[grep("slope", names(scores))]

  overall_tibble <- dplyr::bind_rows(
    scores_to_tibble(mean_scores),
    scores_to_tibble(sd_scores),
    scores_to_tibble(slope_scores)
  )

  overall_tibble <- overall_tibble %>%
    dplyr::mutate(idx = purrr::map2(From, To, ~ data.frame(idx = .x:.y)))

  overall_tibble <- overall_tibble %>%
    tidyr::unnest(cols = c(idx)) %>%
    dplyr::group_by(idx, Type) %>%
    dplyr::summarise(importance = summary_function(importance, ...))

  g <- overall_tibble %>%
    ggplot2::ggplot(ggplot2::aes(x = idx, y = importance, color = Type)) +
    ggplot2::geom_line()

  if (!is.null(optional_example_rownumber)) {
    df <- model$training_df
    X_df <- df[,!colnames(df) == model$target]
    row <- unlist(X_df[optional_example_rownumber,])
    rescaled_row <- scales::rescale(row, to = c(min(overall_tibble$importance),
                                                max(overall_tibble$importance)))

    data <- tibble::tibble(
      idx = 1:length(rescaled_row),
      example = rescaled_row,
      Type = "Example"
    )
    g <- g + geom_line(data = data, aes(x = idx, y = example))
  }
  return(g)
}
